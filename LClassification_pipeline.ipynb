{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " LClassification_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyNdRvCPSHHNtupzpJ2/r4pf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeynepRuveyda/automated_classification/blob/main/LClassification_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AAxFPdSu1XWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916cc50f-8a57-4b71-b03d-da13f20ed81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "9bCg5jYNBBNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install awscli"
      ],
      "metadata": {
        "id": "gRpxFyuQBRNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import awscli"
      ],
      "metadata": {
        "id": "qSBOeBUCBVD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import general Custom Libraries\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "\n",
        "#Manuel dataset imports\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import random\n",
        "import seaborn as sns\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import inspect\n",
        "\n",
        "#Data Generator\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.tensorflow import balanced_batch_generator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "PUUkxo3SBfyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc644cf2-364d-4318-d0d3-0d531504a993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment variables (needs to connect with AWS/ S3)\n",
        "##### After getting access S3 and coco format jsons the following Marc-Antoine codes will be used. They were added as comment.  (Load - Extract - Merge Dataset cells ) "
      ],
      "metadata": {
        "id": "6DJMLvEvBf41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS\n"
      ],
      "metadata": {
        "id": "JnlXR4jHBldB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data \n",
        "DATASET_DIR = \"/content/drive/MyDrive/Automated_Image_Classification/\" "
      ],
      "metadata": {
        "id": "ukb7ErFSCaLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AWS_DATASET_PATH = [\"s3://automi-data-factory/datasets/classif/test1_classif.zip\", \"s3://automi-data-factory/datasets/classif/test2_classif.zip\"]"
      ],
      "metadata": {
        "id": "PZHSa29-Bq2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "qpeSW1kgCNTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def connectS3():\n",
        "    # Connect AWS \n",
        "    # Create Credentials string with keys and region\n",
        "    text_credentials_s3 = f'''\n",
        "    [default]\n",
        "    aws_access_key_id = {AWS_ACCESS_KEY_ID}\n",
        "    aws_secret_access_key = {AWS_SECRET_ACCESS_KEY}\n",
        "    region = {AWS_REGION}\n",
        "    '''\n",
        "    # write the credentials file with above string \n",
        "    with open(PATH_CREDENTIALS_S3, 'w') as f:\n",
        "        f.write(text_credentials_s3)\n",
        "    # export path of above file as environment variable \n",
        "    os.environ['AWS_SHARED_CREDENTIALS_FILE'] = PATH_CREDENTIALS_S3"
      ],
      "metadata": {
        "id": "pV5QcXubCRGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadDataset(dataset_path):\n",
        "    # Verify destination directory exist and create it otherwise\n",
        "    Path(DATASET_DIR).mkdir(exist_ok=True)\n",
        "    # Verify dataset is not already downloaded \n",
        "    dataset_name = Path(dataset_path).name\n",
        "    if not os.path.exists(Path(DATASET_DIR) / dataset_name):\n",
        "        try:\n",
        "            # Download dataset file via command line \n",
        "            os.system(f\"aws s3 cp {dataset_path} {DATASET_DIR}\")\n",
        "        except:\n",
        "            print(f'Unable to load dataset : {dataset_name}')\n",
        "        finally:\n",
        "            # Verify presence and format of file in the DATASET_DIR folder \n",
        "            assert len(list(Path(DATASET_DIR).glob(dataset_name))) == 1"
      ],
      "metadata": {
        "id": "BPAttWuiCZ0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_multiple_datasets(paths : list):\n",
        "  # loop over all paths \n",
        "  for path in paths:\n",
        "      # call the function which download one dataset\n",
        "      loadDataset(path)"
      ],
      "metadata": {
        "id": "vQB-4WgyCqA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "    connectS3()\n",
        "    load_multiple_datasets(AWS_DATASET_PATH)"
      ],
      "metadata": {
        "id": "K0YwuQOMDPen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load()"
      ],
      "metadata": {
        "id": "VHq3lNpTDRcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Dataset"
      ],
      "metadata": {
        "id": "w_VjiBuAw3i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unzipDataset(S3_path : Path):\n",
        "  S3_path = Path(S3_path)\n",
        "  # define path where the zip had been downloaded \n",
        "  local_path = Path(DATASET_DIR) / S3_path.name\n",
        "  # define destination path where the zip will be unarchived\n",
        "  dest_path = Path(DATASET_DIR) \n",
        "  # create destination path \n",
        "  dest_path.mkdir(exist_ok = True)\n",
        "  # unzip file via cmd line and remove zip file \n",
        "  os.system(f\"unzip {local_path} -d {dest_path} && rm -f {local_path}\")\n",
        "  # set up the dest dir with the dataset stem (name without extension)\n",
        "  dest_path = Path(DATASET_DIR)  / S3_path.stem\n",
        "  # Verify architecture \n",
        "  assert (dest_path / 'annotations').exists()\n",
        "  assert (dest_path / 'annotations' / 'instances_default.json').exists() \n",
        "  assert (dest_path / 'images').exists() \n",
        "  assert len(list((dest_path / 'images').glob('*.*'))) > 1"
      ],
      "metadata": {
        "id": "WXG9yCQ5w-4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzipMultipleDatasets(paths : list):\n",
        "    for path in paths:\n",
        "        # for each path passed in argument, call unzipDataset function \n",
        "        unzipDataset(path)"
      ],
      "metadata": {
        "id": "wzJhvL4FxCIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unzipMultipleDatasets(AWS_DATASET_PATH)"
      ],
      "metadata": {
        "id": "xFC07NFCxGEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge Dataset"
      ],
      "metadata": {
        "id": "5DRhac4LxJkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "!pip install pyodi"
      ],
      "metadata": {
        "id": "Or2gd4kDxOWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "from pyodi.apps import coco"
      ],
      "metadata": {
        "id": "bPdCC7zrxRjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getAllImagesWithSameNames(path_ds1, path_ds2):\n",
        "    # get list of img paths in each dataset\n",
        "    img_list_1 = (path_ds1 / 'images').glob('*.*')\n",
        "    img_list_2 = (path_ds2 / 'images').glob('*.*')\n",
        "\n",
        "    # select only img name \n",
        "    img_list_1 = [img.name for img in img_list_1]\n",
        "    img_list_2 = [img.name for img in img_list_2]\n",
        "\n",
        "    # return the intersection between both set of names \n",
        "    return list(set(img_list_1) & set(img_list_2))"
      ],
      "metadata": {
        "id": "J-oewzWDydNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def renameImages(json_path, same_images):\n",
        "    with open(json_path, \"r\") as js_fiile:\n",
        "    # get dictionnary of json file in input \n",
        "        data = json.load(js_fiile)\n",
        "        # extract list of images \n",
        "        json_list_images = data['images']\n",
        "        # find images dir \n",
        "        images_dir = Path(json_path).parents[1] / 'images'\n",
        "        # loop over images with same names \n",
        "        for image_name in same_images:\n",
        "        # loop over list of images in json file  \n",
        "            for i, json_image in enumerate(json_list_images):\n",
        "                # find the one corresponding \n",
        "                if image_name == json_image['file_name']:\n",
        "                    print(image_name)\n",
        "                    image_path = images_dir / image_name\n",
        "                    # change the name by adding \"_1\" at the end of his name \n",
        "                    name, ext = json_image['file_name'].split('.')\n",
        "                    data['images'][i]['file_name'] = \".\".join((name + '_1', ext))\n",
        "                    # rename it in his directory using \"mv\" linux command \n",
        "                    path, ext = str(image_path).rsplit('.',1)\n",
        "                    cmd = f\"mv {image_path} \" + \".\".join((path + '_1', ext))\n",
        "                    os.system(cmd)\n",
        "\n",
        "    # overwrite json file \t\t\n",
        "    with open(json_path, \"w\") as js_file:\n",
        "        json.dump(data, js_file)"
      ],
      "metadata": {
        "id": "6qeWhcZsymgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def move_after_merged(path_ds1, path_ds2, path_ds_dest):\n",
        "    # loop over both dataset path \n",
        "    for path_ds in [path_ds1, path_ds2]:\n",
        "        # loop over image of the current dataset \n",
        "        for img_path in (path_ds / 'images').glob('*.*'):\n",
        "            # copy image to the destination direction \n",
        "            cmd = f\"cp {str(img_path)} {str(path_ds_dest / 'images')}\"\n",
        "            os.system(cmd)"
      ],
      "metadata": {
        "id": "s1I3xHFvyujH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(p1, p2, p_dest):\n",
        "    # verify all images have been merged \n",
        "    assert len(sorted((p1 / 'images').glob('*.*'))) + len(sorted((p2 / 'images').glob('*.*'))) == len(sorted((p_dest / 'images').glob('*.*')))\n",
        "\n",
        "    json1_path = str(p1 / \"annotations\" / \"instances_default.json\")\n",
        "    json2_path = str(p2/ \"annotations\" / \"instances_default.json\")\n",
        "    json_dest_path = str(p_dest / \"annotations\" / \"instances_default.json\")\n",
        "\n",
        "    # Verify all annotation have been merged \n",
        "    with open(json1_path, \"r\") as js1:\n",
        "        # get dictionnary of json file in input \n",
        "        data = json.load(js1)\n",
        "        # extract list of annotations  \n",
        "        num_anns_ds1  = len(data['annotations'])\n",
        "\n",
        "    with open(json2_path, \"r\") as js1:\n",
        "        # get dictionnary of json file in input \n",
        "        data = json.load(js1)\n",
        "        # extract list of annotations  \n",
        "        num_anns_ds2  = len(data['annotations'])\n",
        "\n",
        "\n",
        "    with open(json_dest_path, \"r\") as js1:\n",
        "        # get dictionnary of json file in input \n",
        "        data = json.load(js1)\n",
        "        # extract list of annotations  \n",
        "        num_anns_ds_dest  = len(data['annotations'])\n",
        "\n",
        "    # Verify all annotations have been merged \n",
        "    assert num_anns_ds1 + num_anns_ds2 == num_anns_ds_dest"
      ],
      "metadata": {
        "id": "bIqTfeuczF2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_datasets(path_ds1, path_ds2, dest):\n",
        "    # define destination path\n",
        "    dest_path = Path(DATASET_DIR) / dest\n",
        "    # create directory and subdirectories for merge \n",
        "    dest_path.mkdir(exist_ok = True)\n",
        "    for subdir in ['images', 'labels', 'annotations']:\n",
        "        (dest_path / subdir).mkdir(exist_ok = True)\n",
        "\n",
        "    # define json paths \n",
        "    json1_path = str(path_ds1 / \"annotations\" / \"instances_default.json\")\n",
        "    json2_path = str(path_ds2 / \"annotations\" / \"instances_default.json\")\n",
        "    json_dest_path = str(dest_path / \"annotations\" / \"instances_default.json\")\n",
        "\n",
        "    # find all image with same names \n",
        "    renameImages(json2_path, getAllImagesWithSameNames(path_ds1, path_ds2))\n",
        "    # merge 2 json files using pyodi and define his output name \n",
        "    coco.coco_merge(json1_path, json2_path, json_dest_path)\n",
        "\n",
        "    move_after_merged(path_ds1, path_ds2, dest_path)\n",
        "    test(path_ds1, path_ds2, dest_path)"
      ],
      "metadata": {
        "id": "MB4AiVPjzKLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR"
      ],
      "metadata": {
        "id": "3uTh2vHkzR6t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "61eed5f9-806e-42e4-b3dc-07343dd55890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Automated_Image_Classification/'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DESTINATION = 'dataset'\n",
        "if len(AWS_DATASET_PATH) == 2:\n",
        "  paths = [Path(DATASET_DIR) / Path(p).stem for p in AWS_DATASET_PATH]\n",
        "  merge_datasets(*paths,DESTINATION)"
      ],
      "metadata": {
        "id": "szT7ahZ2zZEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f7de12-5e97-455d-9595-58ec0ee8b659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-08 13:06:36.830 | INFO     | pyodi.apps.coco.coco_merge:coco_merge:51 - Input 1: 4 images, 4 annotations\n",
            "2022-03-08 13:06:36.831 | INFO     | pyodi.apps.coco.coco_merge:coco_merge:51 - Input 2: 4 images, 4 annotations\n",
            "2022-03-08 13:06:36.832 | INFO     | pyodi.apps.coco.coco_merge:coco_merge:89 - Result: 8 images, 8 annotations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AWS_DATASET_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "916ri7PnrU3q",
        "outputId": "73f189d7-38f0-4a8a-fdcf-0c190ed2ad6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['s3://automi-data-factory/datasets/classif/test1_classif.zip',\n",
              " 's3://automi-data-factory/datasets/classif/test2_classif.zip']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###### Convert from COCO to dataframe"
      ],
      "metadata": {
        "id": "BA0ZTSwhCXao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Automated_Image_Classification/Image/testfor_all_1.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")'''"
      ],
      "metadata": {
        "id": "2hCGdYfPHUjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Automated_Image_Classification/dataset/annotations/instances_default.json\") as json_data:\n",
        "    data = json.load(json_data)"
      ],
      "metadata": {
        "id": "bfcphjGpIj5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotations = pd.DataFrame(data['annotations'])"
      ],
      "metadata": {
        "id": "yRHppAoTd-5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotations.drop_duplicates(subset='image_id', keep=\"last\",inplace=True)"
      ],
      "metadata": {
        "id": "DbkCkjpLEynU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "for column_name,columnData in df_annotations['attributes'].iteritems():\n",
        "  labels.append(columnData['label'])"
      ],
      "metadata": {
        "id": "T4GW5FaKEBk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_images = pd.DataFrame(data['images'])"
      ],
      "metadata": {
        "id": "oK2KcOVCNbQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'id': df_images.file_name.to_list(), 'label': labels})"
      ],
      "metadata": {
        "id": "TaaU80AMNjwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "XnZkgVUtfFoi",
        "outputId": "719a777a-84b1-4bb8-adf1-4217f176b1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b1ac387b-777e-4dd5-a297-ad5ebb6629dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DSC_0601.jpg</td>\n",
              "      <td>marking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DSC_0602.jpg</td>\n",
              "      <td>marking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DSC_0618.jpg</td>\n",
              "      <td>no_marking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DSC_0624.jpg</td>\n",
              "      <td>no_marking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LISI-22-012-285476.png</td>\n",
              "      <td>marking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LISI-22-012-288481.png</td>\n",
              "      <td>no_marking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LISI-22-012-293556.png</td>\n",
              "      <td>marking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LISI-22-012-293867.png</td>\n",
              "      <td>no_marking</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1ac387b-777e-4dd5-a297-ad5ebb6629dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b1ac387b-777e-4dd5-a297-ad5ebb6629dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b1ac387b-777e-4dd5-a297-ad5ebb6629dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                       id       label\n",
              "0            DSC_0601.jpg     marking\n",
              "1            DSC_0602.jpg     marking\n",
              "2            DSC_0618.jpg  no_marking\n",
              "3            DSC_0624.jpg  no_marking\n",
              "4  LISI-22-012-285476.png     marking\n",
              "5  LISI-22-012-288481.png  no_marking\n",
              "6  LISI-22-012-293556.png     marking\n",
              "7  LISI-22-012-293867.png  no_marking"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Dataset"
      ],
      "metadata": {
        "id": "54fGcPqSOM_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_train_split(df_dir):\n",
        "    df = pd.read_csv(df_dir)\n",
        "    train_df, test_df = train_test_split(df, test_size=0.1)\n",
        "    parent = Path(df_dir).parent\n",
        "    train_df_path = os.path.join(os.path.expanduser('~'), parent, 'TRAIN_DF.csv')\n",
        "    test_df_path = os.path.join(os.path.expanduser('~'), parent, 'TEST_DF.csv')\n",
        "    train_df.to_csv(train_df_path, index=False)\n",
        "    test_df.to_csv(test_df_path, index=False)\n",
        "    return train_df, test_df"
      ],
      "metadata": {
        "id": "_kjE017pOJDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting data from zip\n",
        "train_df,test_df,path_all_images = create_dataset('/content/drive/MyDrive/Automated_Image_Classification/Image/Data.zip')\n"
      ],
      "metadata": {
        "id": "okdNHQnwCHiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2106e81-e776-4260-ac36-24daeaf50b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 different class folders. If you have more than 2 nm_classes you should split your images as different folders in zip file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train evaluation data splitting\n",
        "train_df, eval_df = train_test_split(train_df, test_size=0.2)"
      ],
      "metadata": {
        "id": "w0FIt0QSeZYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balance Dataset\n",
        "\n",
        "** add if statement for balance and not balance dataset **"
      ],
      "metadata": {
        "id": "uDW1idygNssz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# balance checking\n",
        "def balance_check(train_df):\n",
        "    counts = train_df.label.value_counts()\n",
        "    count_dict = counts.to_dict()\n",
        "    list_count = list(count_dict.values())\n",
        "    if all(x == list_count[0] for x in list_count) == False:\n",
        "        print(\"Training data is not balance.\")\n",
        "        time.sleep(3.0)\n",
        "        return False\n",
        "    else:\n",
        "        print(\"Training data is balance.\")\n",
        "        return True"
      ],
      "metadata": {
        "id": "tKnTt9h4OTz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balance = balance_check(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGkNpo-uXqgy",
        "outputId": "3c22bbbe-b7eb-4b4b-b105-7dcb856706cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data is not balance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class weight for balance\n",
        "\n",
        "def create_class_weight(labels_dict, n_classes):\n",
        "    total = np.sum(list(labels_dict.values()))\n",
        "    keys = labels_dict.keys()\n",
        "    class_weight = dict()\n",
        "    #x = 0\n",
        "    for idx, key in enumerate(keys):\n",
        "        y = labels_dict[key]\n",
        "        score = (total / n_classes) * (1/y)\n",
        "        class_weight[idx] = score\n",
        "        #x += 1\n",
        "    return class_weight"
      ],
      "metadata": {
        "id": "WWJ29-Y8fKj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find class weigths for balancing\n",
        "counts = train_df.label.value_counts()\n",
        "count_dict = counts.to_dict()\n",
        "nm_classes = len(train_df.label.unique().tolist())\n",
        "class_weights = create_class_weight(count_dict,nm_classes)"
      ],
      "metadata": {
        "id": "ZoM5xpyFa7JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random oversampling for balance\n",
        "\n",
        "def random_over_sampling(train_df, all_image_path):\n",
        "    train_df.reset_index(drop=True, inplace=True)\n",
        "    index = train_df.index\n",
        "    counts = train_df.label.value_counts()\n",
        "    count_dict = counts.to_dict()\n",
        "    max_class = max(count_dict.values())\n",
        "    max_key = max(count_dict, key=count_dict.get)\n",
        "    count_dict.pop(max_key)\n",
        "    names = []\n",
        "    labels = []\n",
        "    for key, value in count_dict.items():\n",
        "        dif = max_class - value\n",
        "        label_ind = list(index[train_df[\"label\"] == key])\n",
        "        if len(label_ind) < dif:\n",
        "            inds = random.choices(label_ind, k=dif)\n",
        "            j = 0\n",
        "            for i in inds:\n",
        "                sample = train_df.iloc[i]\n",
        "                names.append(str(j) + '_copy_' + sample['id'])\n",
        "                labels.append((sample['label']))\n",
        "                im = cv2.imread(os.path.join(os.path.expanduser('~'), all_image_path, sample['id']))\n",
        "                cv2.imwrite(os.path.join(os.path.expanduser('~'), all_image_path, str(j) + '_copy_' + sample['id']), im)\n",
        "                j = j + 1\n",
        "        else:\n",
        "            inds = random.sample(label_ind, dif)\n",
        "\n",
        "            for i in inds:\n",
        "                sample = train_df.iloc[i]\n",
        "                names.append('copy_' + sample['id'])\n",
        "                labels.append((sample['label']))\n",
        "                im = cv2.imread(os.path.join(os.path.expanduser('~'), all_image_path, sample['id']), -1)\n",
        "                cv2.imwrite(os.path.join(os.path.expanduser('~'), all_image_path, 'copy_' + sample['id']), im)\n",
        "\n",
        "    new_names = list(train_df.id) + names\n",
        "    new_labels = list(train_df.label) + labels\n",
        "    new_train_df = pd.DataFrame({'id': new_names, 'label': new_labels})\n",
        "    parent = Path(all_image_path).parent\n",
        "    path_csv = os.path.join(os.path.expanduser('~'), parent, 'TRAIN_DF.csv')\n",
        "    new_train_df.to_csv(path_csv, index=False)\n",
        "\n",
        "    return new_train_df"
      ],
      "metadata": {
        "id": "29X1aSlzfunS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random over sampling for balancing\n",
        "if balance == False :\n",
        "  print(\"Class distribution : %s ,Balancing process is started.\" % (count_dict))\n",
        "  train_df = random_over_sampling(train_df,path_all_images)\n",
        "  counts = train_df.label.value_counts()\n",
        "  count_dict = counts.to_dict()\n",
        "  print(\"Class distribution : %s ,Data made balance.\" % (count_dict))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBCq4m-gYG8i",
        "outputId": "efd9656f-4547-491d-eef9-c0bd366b5674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution : {'No T': 159, 'T': 60} ,Balancing process is started.\n",
            "Class distribution : {'No T': 159, 'T': 159} ,Data made balance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generator"
      ],
      "metadata": {
        "id": "aY1_AvITgTAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataGenerator:\n",
        "\n",
        "    def __init__(self, images_folder_dir, test_df, train_df, eval_df, batch_size, target_size):\n",
        "        self.images_folder_dir = images_folder_dir\n",
        "        self.test_df = test_df\n",
        "        self.train_df = train_df\n",
        "        self.eval_df = eval_df\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def data_generator(self):\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            train_datagen = ImageDataGenerator(\n",
        "                rescale=1. / 255,\n",
        "                rotation_range=40,\n",
        "                width_shift_range=0.2,\n",
        "                height_shift_range=0.2,\n",
        "                shear_range=0.2,\n",
        "                zoom_range=0.2,\n",
        "                horizontal_flip=True,\n",
        "                fill_mode='nearest'\n",
        "            )\n",
        "            val_datagen = ImageDataGenerator(\n",
        "                rescale=1. / 255\n",
        "            )\n",
        "            print(\"Test Dataset : \")\n",
        "            test_generator = val_datagen.flow_from_dataframe(\n",
        "                dataframe=self.test_df,\n",
        "                directory=self.images_folder_dir,\n",
        "                x_col=\"id\",\n",
        "                y_col=\"label\",\n",
        "                target_size=self.target_size,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False,\n",
        "                class_mode=\"categorical\"\n",
        "            )\n",
        "            print(\"Validation Dataset : \")\n",
        "            val_generator = val_datagen.flow_from_dataframe(\n",
        "\n",
        "                dataframe=self.eval_df,\n",
        "                directory=self.images_folder_dir,\n",
        "                x_col=\"id\",\n",
        "                y_col=\"label\",\n",
        "                target_size=self.target_size,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True,\n",
        "                seed=42,\n",
        "                class_mode=\"categorical\"\n",
        "            )\n",
        "            print(\"Training Dataset : \")\n",
        "            train_generator = train_datagen.flow_from_dataframe(\n",
        "                dataframe=self.train_df,\n",
        "                directory=self.images_folder_dir,\n",
        "                x_col=\"id\",\n",
        "                y_col=\"label\",\n",
        "                target_size=self.target_size,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True,\n",
        "                seed=42,\n",
        "                class_mode=\"categorical\"\n",
        "            )\n",
        "            return train_generator, val_generator, test_generator\n"
      ],
      "metadata": {
        "id": "ao9THaq8gmFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating custom data generator child from class\n",
        "custom_data = CustomDataGenerator(path_all_images,test_df,train_df,eval_df,32,(224,224))"
      ],
      "metadata": {
        "id": "Se4xtEaBBbKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking balance and creating training,eval,test generators, then creating class weights\n",
        "train_generator,val_generator,test_generator = custom_data.data_generator()"
      ],
      "metadata": {
        "id": "inEwXyYqDRDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c12775-c4a8-4505-ede0-374a258da2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Dataset : \n",
            "Found 31 validated image filenames belonging to 2 classes.\n",
            "Validation Dataset : \n",
            "Found 55 validated image filenames belonging to 2 classes.\n",
            "Training Dataset : \n",
            "Found 318 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='m'></a>\n",
        "## Modeling and Selection"
      ],
      "metadata": {
        "id": "EAfPmemWD7Qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Do you want see all benchmarking of all classification architectures?*** \n",
        "Call benchmarking function and see the result. Then you can choose your final architecture for fine tunning. If you do not want use benchmarking you can directly create model from Custom_Model class then you can apply fine tuning.\n",
        "\n",
        "** add if statement for benchmarking **\n"
      ],
      "metadata": {
        "id": "U9ydljxhCqcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_Model():\n",
        "    def __init__(self, model_name,im_size,nm_classes):\n",
        "\n",
        "        # define all layers in init\n",
        "        self.IMG_SHAPE = im_size + (3,)\n",
        "        self.model_name = model_name\n",
        "        self.nm_classes = nm_classes\n",
        "        self.dense = tf.keras.layers.Dense(self.nm_classes, activation = 'sigmoid')\n",
        "        self.flat = tf.keras.layers.Flatten(name=\"flatten\")\n",
        "        self.model_dictionary = {m[0]: m[1] for m in inspect.getmembers(tf.keras.applications, inspect.isfunction)}\n",
        "        self.base_model = self.model_dictionary[self.model_name](input_shape=self.IMG_SHAPE, include_top=False,\n",
        "                                                                 weights='imagenet')\n",
        "        self.base_model.trainable = False\n",
        "        self.global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "    def forward(self):\n",
        "        input_ = tf.keras.Input(shape=self.IMG_SHAPE)\n",
        "        x = self.base_model(input_,training=False)\n",
        "        x = self.global_average_layer(x)\n",
        "        #x = self.dropout(x)\n",
        "        outputs = self.dense(x)\n",
        "        model = tf.keras.Model(input_,outputs)\n",
        "        \n",
        "        return model"
      ],
      "metadata": {
        "id": "iHPze7-8i4tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_selection(train_generator,validation_generator,im_size,nm_classes):\n",
        "    model_dictionary = {m[0]: m[1] for m in inspect.getmembers(tf.keras.applications, inspect.isfunction)}\n",
        "    model_dictionary.pop('NASNetLarge')\n",
        "    model_benchmarks = {'model_name': [], 'num_model_params': [] ,'validation_accuracy': []}\n",
        "    for model_name, model in tqdm(model_dictionary.items()):\n",
        "\n",
        "        model_ = Custom_Model(model_name,im_size,nm_classes)\n",
        "        # custom modifications on top of pre-trained model\n",
        "        model_ = model_.forward()\n",
        "        base_learning_rate = 0.0001\n",
        "        model_.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        history = model_.fit(train_generator, epochs=3, validation_data=validation_generator,)\n",
        "\n",
        "        model_benchmarks['model_name'].append(model_name)\n",
        "        model_benchmarks['num_model_params'].append(model_.count_params())\n",
        "        model_benchmarks['validation_accuracy'].append(history.history['val_accuracy'][-1])\n",
        "        benchmark_df = pd.DataFrame(model_benchmarks)\n",
        "        benchmark_df.to_csv('benchmark_df.csv', index=False)  # write results to csv file\n",
        "\n",
        "    return benchmark_df"
      ],
      "metadata": {
        "id": "L98wP-PciLSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training all models\n",
        "benchmark_df = model_selection(train_generator,val_generator,(224,224),nm_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypScPOsKFtnc",
        "outputId": "d861436b-30c7-46cd-a722-c6a9f454ec07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/34 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 23s 827ms/step - loss: 0.7671 - accuracy: 0.4403 - val_loss: 0.7561 - val_accuracy: 0.4545\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 567ms/step - loss: 0.7274 - accuracy: 0.4874 - val_loss: 0.7267 - val_accuracy: 0.5091\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 569ms/step - loss: 0.6924 - accuracy: 0.5975 - val_loss: 0.6959 - val_accuracy: 0.5455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 1/34 [00:39<21:48, 39.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "51879936/51877672 [==============================] - 0s 0us/step\n",
            "51888128/51877672 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 16s 840ms/step - loss: 0.6031 - accuracy: 0.8270 - val_loss: 0.5626 - val_accuracy: 0.8000\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 566ms/step - loss: 0.5586 - accuracy: 0.8836 - val_loss: 0.5012 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 563ms/step - loss: 0.5321 - accuracy: 0.9057 - val_loss: 0.4604 - val_accuracy: 0.8909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 2/34 [01:12<19:04, 35.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74842112/74836368 [==============================] - 0s 0us/step\n",
            "74850304/74836368 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 18s 904ms/step - loss: 0.7900 - accuracy: 0.3711 - val_loss: 0.8027 - val_accuracy: 0.3273\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 567ms/step - loss: 0.7484 - accuracy: 0.4214 - val_loss: 0.7635 - val_accuracy: 0.3455\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 580ms/step - loss: 0.6965 - accuracy: 0.5440 - val_loss: 0.7394 - val_accuracy: 0.3818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 3/34 [01:49<18:36, 36.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "16719872/16705208 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 12s 696ms/step - loss: 0.6936 - accuracy: 0.4748 - val_loss: 0.6960 - val_accuracy: 0.1818\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 567ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6974 - val_accuracy: 0.1818\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 572ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6938 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 4/34 [02:15<16:09, 32.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
            "27025408/27018416 [==============================] - 0s 0us/step\n",
            "27033600/27018416 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 15s 821ms/step - loss: 0.6952 - accuracy: 0.5000 - val_loss: 0.6758 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 561ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6851 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 568ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6857 - val_accuracy: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 5/34 [02:47<15:30, 32.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
            "31793152/31790344 [==============================] - 0s 0us/step\n",
            "31801344/31790344 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 15s 775ms/step - loss: 0.6961 - accuracy: 0.5000 - val_loss: 0.7282 - val_accuracy: 0.1818\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 565ms/step - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.7182 - val_accuracy: 0.1818\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 559ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.7061 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 6/34 [03:17<14:43, 31.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "43941888/43941136 [==============================] - 1s 0us/step\n",
            "43950080/43941136 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 17s 822ms/step - loss: 0.6974 - accuracy: 0.5000 - val_loss: 0.6745 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 569ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6877 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 570ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6855 - val_accuracy: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 7/34 [03:51<14:29, 32.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
            "71688192/71686520 [==============================] - 1s 0us/step\n",
            "71696384/71686520 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 19s 880ms/step - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.6678 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 563ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6846 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 571ms/step - loss: 0.6933 - accuracy: 0.4937 - val_loss: 0.7042 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▎       | 8/34 [04:28<14:37, 33.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb5_notop.h5\n",
            "115269632/115263384 [==============================] - 2s 0us/step\n",
            "115277824/115263384 [==============================] - 2s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 22s 921ms/step - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.6632 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 570ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6792 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 573ms/step - loss: 0.6926 - accuracy: 0.5000 - val_loss: 0.6948 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▋       | 9/34 [05:10<15:05, 36.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb6_notop.h5\n",
            "165240832/165234480 [==============================] - 3s 0us/step\n",
            "165249024/165234480 [==============================] - 3s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 23s 1s/step - loss: 0.6973 - accuracy: 0.5000 - val_loss: 0.6686 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 580ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6894 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 572ms/step - loss: 0.6935 - accuracy: 0.4748 - val_loss: 0.7041 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 10/34 [05:56<15:44, 39.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258080768/258076736 [==============================] - 2s 0us/step\n",
            "258088960/258076736 [==============================] - 2s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 29s 1s/step - loss: 0.7017 - accuracy: 0.5000 - val_loss: 0.6574 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 579ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6883 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 581ms/step - loss: 0.6942 - accuracy: 0.4811 - val_loss: 0.7098 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 11/34 [06:47<16:26, 42.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "24281088/24274472 [==============================] - 0s 0us/step\n",
            "24289280/24274472 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 12s 727ms/step - loss: 0.6954 - accuracy: 0.4686 - val_loss: 0.6759 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 556ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6839 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 556ms/step - loss: 0.6941 - accuracy: 0.4434 - val_loss: 0.6986 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 12/34 [07:14<13:59, 38.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b1_notop.h5\n",
            "28459008/28456008 [==============================] - 1s 0us/step\n",
            "28467200/28456008 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 14s 774ms/step - loss: 0.6980 - accuracy: 0.5000 - val_loss: 0.6558 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 556ms/step - loss: 0.6951 - accuracy: 0.5000 - val_loss: 0.6808 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 566ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 13/34 [07:46<12:42, 36.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b2_notop.h5\n",
            "35840000/35839040 [==============================] - 1s 0us/step\n",
            "35848192/35839040 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 15s 786ms/step - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.7161 - val_accuracy: 0.1818\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 558ms/step - loss: 0.6949 - accuracy: 0.4340 - val_loss: 0.6900 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 554ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6900 - val_accuracy: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 14/34 [08:17<11:32, 34.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b3_notop.h5\n",
            "52609024/52606240 [==============================] - 1s 0us/step\n",
            "52617216/52606240 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 16s 829ms/step - loss: 0.6956 - accuracy: 0.5000 - val_loss: 0.6756 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 564ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6860 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 568ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6885 - val_accuracy: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 15/34 [08:51<10:51, 34.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n",
            "473178112/473176280 [==============================] - 9s 0us/step\n",
            "473186304/473176280 [==============================] - 9s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 33s 1s/step - loss: 0.6934 - accuracy: 0.5126 - val_loss: 0.6973 - val_accuracy: 0.2000\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 587ms/step - loss: 0.6931 - accuracy: 0.5597 - val_loss: 0.6940 - val_accuracy: 0.4182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 587ms/step - loss: 0.6923 - accuracy: 0.6038 - val_loss: 0.6921 - val_accuracy: 0.6545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 16/34 [09:56<13:04, 43.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-m_notop.h5\n",
            "214204416/214201816 [==============================] - 4s 0us/step\n",
            "214212608/214201816 [==============================] - 4s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 26s 1s/step - loss: 0.6947 - accuracy: 0.4717 - val_loss: 0.6884 - val_accuracy: 0.8000\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 615ms/step - loss: 0.6941 - accuracy: 0.5252 - val_loss: 0.6885 - val_accuracy: 0.7818\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 571ms/step - loss: 0.6941 - accuracy: 0.5126 - val_loss: 0.6878 - val_accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 17/34 [10:45<12:50, 45.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-s_notop.h5\n",
            "82427904/82420632 [==============================] - 2s 0us/step\n",
            "82436096/82420632 [==============================] - 2s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 18s 882ms/step - loss: 0.6979 - accuracy: 0.4057 - val_loss: 0.6789 - val_accuracy: 0.6909\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 570ms/step - loss: 0.6939 - accuracy: 0.4403 - val_loss: 0.6577 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 572ms/step - loss: 0.6889 - accuracy: 0.4937 - val_loss: 0.6403 - val_accuracy: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 18/34 [11:23<11:28, 43.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n",
            "219070464/219055592 [==============================] - 2s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 19s 936ms/step - loss: 0.6600 - accuracy: 0.7925 - val_loss: 0.5789 - val_accuracy: 0.7818\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 568ms/step - loss: 0.5825 - accuracy: 0.8491 - val_loss: 0.5284 - val_accuracy: 0.8727\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 568ms/step - loss: 0.5224 - accuracy: 0.9434 - val_loss: 0.4731 - val_accuracy: 0.9455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 19/34 [12:04<10:35, 42.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 11s 719ms/step - loss: 0.6674 - accuracy: 0.6667 - val_loss: 0.6204 - val_accuracy: 0.8000\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 565ms/step - loss: 0.5872 - accuracy: 0.7862 - val_loss: 0.5214 - val_accuracy: 0.8909\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 570ms/step - loss: 0.5290 - accuracy: 0.8648 - val_loss: 0.4754 - val_accuracy: 0.8727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 20/34 [12:30<08:46, 37.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "17235968/17225924 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 8s 642ms/step - loss: 0.8712 - accuracy: 0.4403 - val_loss: 0.6747 - val_accuracy: 0.6545\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 565ms/step - loss: 0.7737 - accuracy: 0.4528 - val_loss: 0.6602 - val_accuracy: 0.6364\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 559ms/step - loss: 0.7091 - accuracy: 0.5440 - val_loss: 0.6237 - val_accuracy: 0.6909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 21/34 [12:51<07:02, 32.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 9s 656ms/step - loss: 0.7418 - accuracy: 0.4591 - val_loss: 0.5757 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 5s 552ms/step - loss: 0.6641 - accuracy: 0.6258 - val_loss: 0.5637 - val_accuracy: 0.8364\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 5s 553ms/step - loss: 0.6273 - accuracy: 0.7138 - val_loss: 0.5404 - val_accuracy: 0.8727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▍   | 22/34 [13:13<05:52, 29.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n",
            "12689408/12683000 [==============================] - 1s 0us/step\n",
            "12697600/12683000 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 10s 677ms/step - loss: 0.7033 - accuracy: 0.5000 - val_loss: 0.7250 - val_accuracy: 0.1818\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 559ms/step - loss: 0.7013 - accuracy: 0.5000 - val_loss: 0.7193 - val_accuracy: 0.1818\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 553ms/step - loss: 0.6994 - accuracy: 0.5000 - val_loss: 0.7168 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 23/34 [13:39<05:14, 28.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n",
            "4341760/4334752 [==============================] - 0s 0us/step\n",
            "4349952/4334752 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 9s 669ms/step - loss: 0.7032 - accuracy: 0.5000 - val_loss: 0.7815 - val_accuracy: 0.1818\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 554ms/step - loss: 0.6996 - accuracy: 0.5000 - val_loss: 0.7609 - val_accuracy: 0.1818\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 5s 554ms/step - loss: 0.6970 - accuracy: 0.5000 - val_loss: 0.7430 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 24/34 [14:02<04:27, 26.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile-no-top.h5\n",
            "19996672/19993432 [==============================] - 1s 0us/step\n",
            "20004864/19993432 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 22s 980ms/step - loss: 0.7434 - accuracy: 0.5314 - val_loss: 0.7556 - val_accuracy: 0.3818\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 564ms/step - loss: 0.7031 - accuracy: 0.5975 - val_loss: 0.7125 - val_accuracy: 0.4364\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 559ms/step - loss: 0.6731 - accuracy: 0.6509 - val_loss: 0.6747 - val_accuracy: 0.6545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▎  | 25/34 [14:44<04:41, 31.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171450368/171446536 [==============================] - 4s 0us/step\n",
            "171458560/171446536 [==============================] - 4s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 13s 777ms/step - loss: 0.7052 - accuracy: 0.5000 - val_loss: 0.7811 - val_accuracy: 0.1818\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 562ms/step - loss: 0.7018 - accuracy: 0.5000 - val_loss: 0.7366 - val_accuracy: 0.1818\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 558ms/step - loss: 0.6976 - accuracy: 0.5000 - val_loss: 0.7122 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▋  | 26/34 [15:16<04:12, 31.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171319296/171317808 [==============================] - 3s 0us/step\n",
            "171327488/171317808 [==============================] - 3s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 13s 758ms/step - loss: 0.8515 - accuracy: 0.4119 - val_loss: 0.6173 - val_accuracy: 0.6909\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 559ms/step - loss: 0.7474 - accuracy: 0.5566 - val_loss: 0.5824 - val_accuracy: 0.7818\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 568ms/step - loss: 0.6805 - accuracy: 0.6164 - val_loss: 0.5292 - val_accuracy: 0.8727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 27/34 [15:48<03:41, 31.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234700800/234698864 [==============================] - 4s 0us/step\n",
            "234708992/234698864 [==============================] - 4s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 17s 886ms/step - loss: 0.8143 - accuracy: 0.5000 - val_loss: 0.5542 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 567ms/step - loss: 0.7851 - accuracy: 0.5000 - val_loss: 0.5720 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 578ms/step - loss: 0.7621 - accuracy: 0.5000 - val_loss: 0.5887 - val_accuracy: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 28/34 [16:27<03:22, 33.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234553344/234545216 [==============================] - 3s 0us/step\n",
            "234561536/234545216 [==============================] - 3s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 16s 850ms/step - loss: 1.0172 - accuracy: 0.4497 - val_loss: 0.6702 - val_accuracy: 0.6909\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 572ms/step - loss: 0.8857 - accuracy: 0.4528 - val_loss: 0.6805 - val_accuracy: 0.5273\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 574ms/step - loss: 0.7935 - accuracy: 0.5000 - val_loss: 0.6700 - val_accuracy: 0.6182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 29/34 [17:02<02:51, 34.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 10s 690ms/step - loss: 0.7166 - accuracy: 0.4748 - val_loss: 0.6916 - val_accuracy: 0.7636\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 558ms/step - loss: 0.7048 - accuracy: 0.4591 - val_loss: 0.6867 - val_accuracy: 0.7636\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 561ms/step - loss: 0.6994 - accuracy: 0.4686 - val_loss: 0.6822 - val_accuracy: 0.7636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 30/34 [17:27<02:05, 31.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 1s 0us/step\n",
            "94683136/94668760 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 9s 669ms/step - loss: 0.6361 - accuracy: 0.7107 - val_loss: 0.5279 - val_accuracy: 0.8364\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 557ms/step - loss: 0.5544 - accuracy: 0.7925 - val_loss: 0.4969 - val_accuracy: 0.8909\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 558ms/step - loss: 0.4520 - accuracy: 0.9340 - val_loss: 0.4595 - val_accuracy: 0.9273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 31/34 [17:52<01:28, 29.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 8s 716ms/step - loss: 0.7965 - accuracy: 0.5000 - val_loss: 0.4906 - val_accuracy: 0.8182\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 557ms/step - loss: 0.7846 - accuracy: 0.5000 - val_loss: 0.4968 - val_accuracy: 0.8182\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 553ms/step - loss: 0.7688 - accuracy: 0.5000 - val_loss: 0.5031 - val_accuracy: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 32/34 [18:13<00:53, 26.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 7s 609ms/step - loss: 0.8158 - accuracy: 0.5000 - val_loss: 1.0154 - val_accuracy: 0.1818\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 565ms/step - loss: 0.8022 - accuracy: 0.5000 - val_loss: 0.9925 - val_accuracy: 0.1818\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 563ms/step - loss: 0.7906 - accuracy: 0.5000 - val_loss: 0.9679 - val_accuracy: 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 33/34 [18:34<00:25, 25.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n",
            "83697664/83683744 [==============================] - 1s 0us/step\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 9s 679ms/step - loss: 0.7232 - accuracy: 0.4151 - val_loss: 0.7339 - val_accuracy: 0.4364\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 6s 565ms/step - loss: 0.6810 - accuracy: 0.6258 - val_loss: 0.6781 - val_accuracy: 0.6364\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 563ms/step - loss: 0.6388 - accuracy: 0.7862 - val_loss: 0.6302 - val_accuracy: 0.8364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [18:57<00:00, 33.46s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Results to DataFrame for easy viewing\n",
        "benchmark_df = pd.DataFrame(benchmark_df)\n",
        "benchmark_df.sort_values('num_model_params', inplace=True) # sort in ascending order of num_model_params column\n",
        "benchmark_df.to_csv('benchmark_df.csv', index=False) # write results to csv file\n",
        "benchmark_df_sorted = benchmark_df.sort_values([\"validation_accuracy\"], ascending=False)\n",
        "benchmark_df_sorted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gLkVm8_ymjp4",
        "outputId": "21f2a66f-e3df-486c-a731-f923dc73785d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-54626fee-a28c-4d71-ba8a-960f01a8dc28\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>num_model_params</th>\n",
              "      <th>validation_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>InceptionResNetV2</td>\n",
              "      <td>54339810</td>\n",
              "      <td>0.945455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>ResNet50V2</td>\n",
              "      <td>23568898</td>\n",
              "      <td>0.927273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DenseNet169</td>\n",
              "      <td>12646210</td>\n",
              "      <td>0.890909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>InceptionV3</td>\n",
              "      <td>21806882</td>\n",
              "      <td>0.872727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MobileNetV2</td>\n",
              "      <td>2260546</td>\n",
              "      <td>0.872727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ResNet101V2</td>\n",
              "      <td>42630658</td>\n",
              "      <td>0.872727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Xception</td>\n",
              "      <td>20865578</td>\n",
              "      <td>0.836364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>EfficientNetV2S</td>\n",
              "      <td>20333922</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VGG16</td>\n",
              "      <td>14715714</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>EfficientNetV2B3</td>\n",
              "      <td>12933696</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>EfficientNetB3</td>\n",
              "      <td>10786609</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>EfficientNetV2B2</td>\n",
              "      <td>8772192</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>EfficientNetV2B1</td>\n",
              "      <td>6933686</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EfficientNetB1</td>\n",
              "      <td>6577801</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>ResNet152</td>\n",
              "      <td>58375042</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>EfficientNetV2M</td>\n",
              "      <td>53152950</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ResNet50</td>\n",
              "      <td>23591810</td>\n",
              "      <td>0.763636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MobileNet</td>\n",
              "      <td>3230914</td>\n",
              "      <td>0.690909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NASNetMobile</td>\n",
              "      <td>4271830</td>\n",
              "      <td>0.654545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>EfficientNetV2L</td>\n",
              "      <td>117749410</td>\n",
              "      <td>0.654545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ResNet152V2</td>\n",
              "      <td>58335746</td>\n",
              "      <td>0.618182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DenseNet121</td>\n",
              "      <td>7039554</td>\n",
              "      <td>0.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DenseNet201</td>\n",
              "      <td>18325826</td>\n",
              "      <td>0.381818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>VGG19</td>\n",
              "      <td>20025410</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>EfficientNetB5</td>\n",
              "      <td>28517625</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>EfficientNetB6</td>\n",
              "      <td>40964753</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EfficientNetB4</td>\n",
              "      <td>17677409</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ResNet101</td>\n",
              "      <td>42662274</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>EfficientNetB2</td>\n",
              "      <td>7771387</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>EfficientNetV2B0</td>\n",
              "      <td>5921874</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EfficientNetB0</td>\n",
              "      <td>4052133</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>MobileNetV3Large</td>\n",
              "      <td>2998274</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>EfficientNetB7</td>\n",
              "      <td>64102809</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>MobileNetV3Small</td>\n",
              "      <td>940274</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54626fee-a28c-4d71-ba8a-960f01a8dc28')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-54626fee-a28c-4d71-ba8a-960f01a8dc28 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-54626fee-a28c-4d71-ba8a-960f01a8dc28');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           model_name  num_model_params  validation_accuracy\n",
              "18  InceptionResNetV2          54339810             0.945455\n",
              "30         ResNet50V2          23568898             0.927273\n",
              "1         DenseNet169          12646210             0.890909\n",
              "19        InceptionV3          21806882             0.872727\n",
              "21        MobileNetV2           2260546             0.872727\n",
              "26        ResNet101V2          42630658             0.872727\n",
              "33           Xception          20865578             0.836364\n",
              "17    EfficientNetV2S          20333922             0.818182\n",
              "31              VGG16          14715714             0.818182\n",
              "14   EfficientNetV2B3          12933696             0.818182\n",
              "6      EfficientNetB3          10786609             0.818182\n",
              "13   EfficientNetV2B2           8772192             0.818182\n",
              "12   EfficientNetV2B1           6933686             0.818182\n",
              "4      EfficientNetB1           6577801             0.818182\n",
              "27          ResNet152          58375042             0.818182\n",
              "16    EfficientNetV2M          53152950             0.800000\n",
              "29           ResNet50          23591810             0.763636\n",
              "20          MobileNet           3230914             0.690909\n",
              "24       NASNetMobile           4271830             0.654545\n",
              "15    EfficientNetV2L         117749410             0.654545\n",
              "28        ResNet152V2          58335746             0.618182\n",
              "0         DenseNet121           7039554             0.545455\n",
              "2         DenseNet201          18325826             0.381818\n",
              "32              VGG19          20025410             0.181818\n",
              "8      EfficientNetB5          28517625             0.181818\n",
              "9      EfficientNetB6          40964753             0.181818\n",
              "7      EfficientNetB4          17677409             0.181818\n",
              "25          ResNet101          42662274             0.181818\n",
              "5      EfficientNetB2           7771387             0.181818\n",
              "11   EfficientNetV2B0           5921874             0.181818\n",
              "3      EfficientNetB0           4052133             0.181818\n",
              "22   MobileNetV3Large           2998274             0.181818\n",
              "10     EfficientNetB7          64102809             0.181818\n",
              "23   MobileNetV3Small            940274             0.181818"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working Specific Model"
      ],
      "metadata": {
        "id": "bWF1T79_43T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Custom_Model('InceptionResNetV2',(224,224),nm_classes)"
      ],
      "metadata": {
        "id": "aUbzyKDSPmCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.forward()"
      ],
      "metadata": {
        "id": "Xcn8dk9zj3vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earlystopping = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience=20,min_delta=0.001)\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\", verbose=1, save_best_only=True)"
      ],
      "metadata": {
        "id": "zCvYMJNJ0m3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 0.001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wjPtM9XwXahJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RWay5gNqxGY",
        "outputId": "7277749e-66ae-4a0f-bcd5-980b2a05f90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_72 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " inception_resnet_v2 (Functi  (None, 5, 5, 1536)       54336736  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " global_average_pooling2d_35  (None, 1536)             0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 2)                 3074      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,339,810\n",
            "Trainable params: 3,074\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,epochs=50,validation_data=val_generator,class_weight=class_weights, callbacks=[checkpointer, earlystopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXejfen2EEmG",
        "outputId": "5a68795d-0903-49bb-97c9-565a5e07d798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7417 - accuracy: 0.5377\n",
            "Epoch 1: val_loss improved from inf to 0.59102, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.7417 - accuracy: 0.5377 - val_loss: 0.5910 - val_accuracy: 0.8727\n",
            "Epoch 2/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.9182\n",
            "Epoch 2: val_loss improved from 0.59102 to 0.35634, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 8s 753ms/step - loss: 0.3679 - accuracy: 0.9182 - val_loss: 0.3563 - val_accuracy: 0.8909\n",
            "Epoch 3/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9623\n",
            "Epoch 3: val_loss improved from 0.35634 to 0.22828, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 752ms/step - loss: 0.2069 - accuracy: 0.9623 - val_loss: 0.2283 - val_accuracy: 0.9273\n",
            "Epoch 4/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9623\n",
            "Epoch 4: val_loss improved from 0.22828 to 0.18328, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 742ms/step - loss: 0.1473 - accuracy: 0.9623 - val_loss: 0.1833 - val_accuracy: 0.9273\n",
            "Epoch 5/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9686\n",
            "Epoch 5: val_loss improved from 0.18328 to 0.14538, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 747ms/step - loss: 0.1292 - accuracy: 0.9686 - val_loss: 0.1454 - val_accuracy: 0.9455\n",
            "Epoch 6/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9780\n",
            "Epoch 6: val_loss improved from 0.14538 to 0.11252, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 746ms/step - loss: 0.0922 - accuracy: 0.9780 - val_loss: 0.1125 - val_accuracy: 0.9636\n",
            "Epoch 7/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9811\n",
            "Epoch 7: val_loss improved from 0.11252 to 0.10861, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 8s 774ms/step - loss: 0.0838 - accuracy: 0.9811 - val_loss: 0.1086 - val_accuracy: 0.9636\n",
            "Epoch 8/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9748\n",
            "Epoch 8: val_loss improved from 0.10861 to 0.09703, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 8s 825ms/step - loss: 0.0802 - accuracy: 0.9748 - val_loss: 0.0970 - val_accuracy: 0.9818\n",
            "Epoch 9/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9937\n",
            "Epoch 9: val_loss did not improve from 0.09703\n",
            "10/10 [==============================] - 6s 576ms/step - loss: 0.0678 - accuracy: 0.9937 - val_loss: 0.1009 - val_accuracy: 0.9818\n",
            "Epoch 10/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9843\n",
            "Epoch 10: val_loss improved from 0.09703 to 0.09045, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 8s 779ms/step - loss: 0.0654 - accuracy: 0.9843 - val_loss: 0.0905 - val_accuracy: 0.9818\n",
            "Epoch 11/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9906\n",
            "Epoch 11: val_loss improved from 0.09045 to 0.07323, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 744ms/step - loss: 0.0574 - accuracy: 0.9906 - val_loss: 0.0732 - val_accuracy: 0.9818\n",
            "Epoch 12/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9748\n",
            "Epoch 12: val_loss improved from 0.07323 to 0.06735, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 743ms/step - loss: 0.0626 - accuracy: 0.9748 - val_loss: 0.0673 - val_accuracy: 0.9818\n",
            "Epoch 13/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9874\n",
            "Epoch 13: val_loss did not improve from 0.06735\n",
            "10/10 [==============================] - 6s 577ms/step - loss: 0.0630 - accuracy: 0.9874 - val_loss: 0.0945 - val_accuracy: 0.9636\n",
            "Epoch 14/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9811\n",
            "Epoch 14: val_loss did not improve from 0.06735\n",
            "10/10 [==============================] - 6s 591ms/step - loss: 0.0485 - accuracy: 0.9811 - val_loss: 0.0765 - val_accuracy: 0.9818\n",
            "Epoch 15/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9874\n",
            "Epoch 15: val_loss did not improve from 0.06735\n",
            "10/10 [==============================] - 6s 570ms/step - loss: 0.0529 - accuracy: 0.9874 - val_loss: 0.0743 - val_accuracy: 0.9818\n",
            "Epoch 16/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9937\n",
            "Epoch 16: val_loss improved from 0.06735 to 0.06556, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 9s 910ms/step - loss: 0.0380 - accuracy: 0.9937 - val_loss: 0.0656 - val_accuracy: 0.9818\n",
            "Epoch 17/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9843\n",
            "Epoch 17: val_loss improved from 0.06556 to 0.05831, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 751ms/step - loss: 0.0538 - accuracy: 0.9843 - val_loss: 0.0583 - val_accuracy: 0.9818\n",
            "Epoch 18/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9969\n",
            "Epoch 18: val_loss did not improve from 0.05831\n",
            "10/10 [==============================] - 6s 581ms/step - loss: 0.0445 - accuracy: 0.9969 - val_loss: 0.0701 - val_accuracy: 0.9818\n",
            "Epoch 19/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9937\n",
            "Epoch 19: val_loss did not improve from 0.05831\n",
            "10/10 [==============================] - 6s 592ms/step - loss: 0.0385 - accuracy: 0.9937 - val_loss: 0.0629 - val_accuracy: 0.9818\n",
            "Epoch 20/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9937\n",
            "Epoch 20: val_loss improved from 0.05831 to 0.05167, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 9s 885ms/step - loss: 0.0391 - accuracy: 0.9937 - val_loss: 0.0517 - val_accuracy: 0.9818\n",
            "Epoch 21/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9874\n",
            "Epoch 21: val_loss did not improve from 0.05167\n",
            "10/10 [==============================] - 6s 577ms/step - loss: 0.0402 - accuracy: 0.9874 - val_loss: 0.0521 - val_accuracy: 0.9818\n",
            "Epoch 22/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9969\n",
            "Epoch 22: val_loss improved from 0.05167 to 0.04680, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 8s 826ms/step - loss: 0.0277 - accuracy: 0.9969 - val_loss: 0.0468 - val_accuracy: 0.9818\n",
            "Epoch 23/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9874\n",
            "Epoch 23: val_loss improved from 0.04680 to 0.04126, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 734ms/step - loss: 0.0352 - accuracy: 0.9874 - val_loss: 0.0413 - val_accuracy: 0.9818\n",
            "Epoch 24/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9937\n",
            "Epoch 24: val_loss did not improve from 0.04126\n",
            "10/10 [==============================] - 6s 573ms/step - loss: 0.0367 - accuracy: 0.9937 - val_loss: 0.0457 - val_accuracy: 0.9818\n",
            "Epoch 25/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9937\n",
            "Epoch 25: val_loss did not improve from 0.04126\n",
            "10/10 [==============================] - 6s 594ms/step - loss: 0.0370 - accuracy: 0.9937 - val_loss: 0.0533 - val_accuracy: 0.9818\n",
            "Epoch 26/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9937\n",
            "Epoch 26: val_loss did not improve from 0.04126\n",
            "10/10 [==============================] - 6s 560ms/step - loss: 0.0380 - accuracy: 0.9937 - val_loss: 0.0558 - val_accuracy: 0.9818\n",
            "Epoch 27/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9969\n",
            "Epoch 27: val_loss improved from 0.04126 to 0.03864, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 9s 906ms/step - loss: 0.0334 - accuracy: 0.9969 - val_loss: 0.0386 - val_accuracy: 0.9818\n",
            "Epoch 28/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9937\n",
            "Epoch 28: val_loss improved from 0.03864 to 0.03808, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 744ms/step - loss: 0.0325 - accuracy: 0.9937 - val_loss: 0.0381 - val_accuracy: 0.9818\n",
            "Epoch 29/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9969\n",
            "Epoch 29: val_loss improved from 0.03808 to 0.03680, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 738ms/step - loss: 0.0340 - accuracy: 0.9969 - val_loss: 0.0368 - val_accuracy: 0.9818\n",
            "Epoch 30/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9969\n",
            "Epoch 30: val_loss improved from 0.03680 to 0.03448, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 738ms/step - loss: 0.0282 - accuracy: 0.9969 - val_loss: 0.0345 - val_accuracy: 0.9818\n",
            "Epoch 31/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9874\n",
            "Epoch 31: val_loss did not improve from 0.03448\n",
            "10/10 [==============================] - 6s 578ms/step - loss: 0.0292 - accuracy: 0.9874 - val_loss: 0.0370 - val_accuracy: 0.9818\n",
            "Epoch 32/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9906\n",
            "Epoch 32: val_loss improved from 0.03448 to 0.03345, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 752ms/step - loss: 0.0275 - accuracy: 0.9906 - val_loss: 0.0335 - val_accuracy: 0.9818\n",
            "Epoch 33/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9937\n",
            "Epoch 33: val_loss did not improve from 0.03345\n",
            "10/10 [==============================] - 6s 566ms/step - loss: 0.0410 - accuracy: 0.9937 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9906\n",
            "Epoch 34: val_loss did not improve from 0.03345\n",
            "10/10 [==============================] - 6s 579ms/step - loss: 0.0235 - accuracy: 0.9906 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9969\n",
            "Epoch 35: val_loss improved from 0.03345 to 0.03314, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 8s 877ms/step - loss: 0.0276 - accuracy: 0.9969 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9937\n",
            "Epoch 36: val_loss did not improve from 0.03314\n",
            "10/10 [==============================] - 6s 568ms/step - loss: 0.0301 - accuracy: 0.9937 - val_loss: 0.0363 - val_accuracy: 0.9818\n",
            "Epoch 37/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9937\n",
            "Epoch 37: val_loss improved from 0.03314 to 0.02365, saving model to /content/drive/MyDrive/Automated_Image_Classification/first_training.hdf5\n",
            "10/10 [==============================] - 7s 753ms/step - loss: 0.0244 - accuracy: 0.9937 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9937\n",
            "Epoch 38: val_loss did not improve from 0.02365\n",
            "10/10 [==============================] - 6s 572ms/step - loss: 0.0255 - accuracy: 0.9937 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
            "Epoch 38: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics Visualization"
      ],
      "metadata": {
        "id": "RFvbNrSemR4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_visualization(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()), 1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.ylim([0, 1.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "\n",
        "    return plt.show()"
      ],
      "metadata": {
        "id": "VuZB_NjKmWQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_visualization(history)"
      ],
      "metadata": {
        "id": "t-KP61a3INY2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "9656a5bd-6001-46f0-9966-9c9b0a995403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Prediction\n"
      ],
      "metadata": {
        "id": "YlaU2hBB6T_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
        "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
        "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
        "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
        "\n",
        "  print(\"_______________________________________________________________________\")\n",
        "  print(classification_report(labels, predictions))\n",
        "\n",
        "  return plt.show"
      ],
      "metadata": {
        "id": "fpmjZkbRmfzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = test_generator.classes\n",
        "predict = model.predict_generator(test_generator)\n",
        "y_pred = np.argmax(predict, axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9cRaw8c-aEw",
        "outputId": "116f3045-fe21-4a1d-bebb-4dc6e349c041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting confusion matrix and classfication report\n",
        "plot_cm(y_true,y_pred)"
      ],
      "metadata": {
        "id": "cMKW93pg3-ZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "outputId": "4a807e65-1d2b-46d3-84f8-ef7611e54bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Legitimate Transactions Detected (True Negatives):  25\n",
            "Legitimate Transactions Incorrectly Detected (False Positives):  0\n",
            "Fraudulent Transactions Missed (False Negatives):  0\n",
            "Fraudulent Transactions Detected (True Positives):  6\n",
            "Total Fraudulent Transactions:  6\n",
            "_______________________________________________________________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        25\n",
            "           1       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           1.00        31\n",
            "   macro avg       1.00      1.00      1.00        31\n",
            "weighted avg       1.00      1.00      1.00        31\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sJKEo4gCJGM6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}